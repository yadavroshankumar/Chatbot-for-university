{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yadavroshankumar/Chatbot-for-university/blob/main/CSIT599_Module1_In_Class_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Classification with Single Layer and Multi-Layer Perceptrons\n",
        "## Exercise for Students\n",
        "\n",
        "\n",
        "This exercise demonstrates the difference between Single Layer Perceptron (SLP)\n",
        "and Multi-Layer Perceptron (MLP) for handwritten digit classification.\n",
        "\n",
        "\n",
        "Instructions:\n",
        "1. Fill in the blanks marked with \"# TODO: STUDENT FILL IN\"\n",
        "2. Run the code and compare the performance of both models\n",
        "3. Experiment with different architectures and hyperparameters"
      ],
      "metadata": {
        "id": "bXH7ACB6h00z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAzhOXG7hgtO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Instructions:\n",
        "1. Fill in the blanks marked with \"# TODO: STUDENT FILL IN\"\n",
        "2. Run the code and compare the performance of both models\n",
        "3. Experiment with different architectures and hyperparameters\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"\n",
        "    Load MNIST dataset and preprocess it for neural network training.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (x_train, y_train, x_test, y_test) - preprocessed data\n",
        "    \"\"\"\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "\n",
        "    # Load the MNIST dataset (built into Keras)\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    print(f\"Training data shape: {x_train.shape}\")\n",
        "    print(f\"Training labels shape: {y_train.shape}\")\n",
        "    print(f\"Test data shape: {x_test.shape}\")\n",
        "    print(f\"Test labels shape: {y_test.shape}\")\n",
        "    print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
        "\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Normalize pixel values to range [0, 1] by dividing by 255.0\n",
        "    x_train = x_train / ________\n",
        "    x_test = x_test / ________\n",
        "\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Reshape images from 28x28 to 784 (flatten for perceptron input)\n",
        "    # Hint: use reshape(-1, 28*28) where -1 means \"infer this dimension\"\n",
        "    x_train = x_train.reshape(________, ________)\n",
        "    x_test = x_test.reshape(________, ________)\n",
        "\n",
        "    # Convert labels to categorical (one-hot encoding)\n",
        "    # This converts integer labels (0-9) to binary vectors\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    print(f\"After preprocessing:\")\n",
        "    print(f\"Training data shape: {x_train.shape}\")\n",
        "    print(f\"Training labels shape: {y_train.shape}\")\n",
        "    print(f\"Pixel value range: [{x_train.min():.2f}, {x_train.max():.2f}]\")\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 2: SINGLE LAYER PERCEPTRON (SLP)\n",
        "# ============================================================================\n",
        "\n",
        "def create_single_layer_perceptron():\n",
        "    \"\"\"\n",
        "    Create a Single Layer Perceptron model.\n",
        "\n",
        "    A Single Layer Perceptron consists of:\n",
        "    - Input layer (784 neurons for 28x28 flattened images)\n",
        "    - Output layer (10 neurons for 10 digit classes)\n",
        "    - No hidden layers!\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled SLP model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CREATING SINGLE LAYER PERCEPTRON\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model = keras.Sequential(name=\"Single_Layer_Perceptron\")\n",
        "\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Add a single Dense layer with 10 units (for 10 classes)\n",
        "    # Use 'softmax' activation for multi-class classification\n",
        "    # Specify input_shape=(784,) for the flattened 28x28 images\n",
        "    model.add(layers.Dense(\n",
        "        units=________,\n",
        "        activation='________',\n",
        "        input_shape=(________,),\n",
        "        name='output_layer'\n",
        "    ))\n",
        "\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Compile the model:\n",
        "    # - optimizer: 'adam' (adaptive learning rate optimizer)\n",
        "    # - loss: 'categorical_crossentropy' (for multi-class classification)\n",
        "    # - metrics: ['accuracy'] to track accuracy during training\n",
        "    model.compile(\n",
        "        optimizer='________',\n",
        "        loss='________',\n",
        "        metrics=['________']\n",
        "    )\n",
        "\n",
        "    # Display model architecture\n",
        "    print(\"\\nSingle Layer Perceptron Architecture:\")\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "mvJjg50Uhkyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 3: MULTI-LAYER PERCEPTRON (MLP)\n",
        "# ============================================================================\n",
        "\n",
        "def create_multi_layer_perceptron():\n",
        "    \"\"\"\n",
        "    Create a Multi-Layer Perceptron model.\n",
        "\n",
        "    A Multi-Layer Perceptron consists of:\n",
        "    - Input layer (784 neurons)\n",
        "    - Hidden layer(s) with non-linear activation\n",
        "    - Output layer (10 neurons)\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled MLP model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CREATING MULTI-LAYER PERCEPTRON\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model = keras.Sequential(name=\"Multi_Layer_Perceptron\")\n",
        "\n",
        "    # Hidden Layer 1\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Add first hidden layer with 128 units and 'relu' activation\n",
        "    # Specify input_shape=(784,) for the first layer\n",
        "    model.add(layers.Dense(\n",
        "        units=________,\n",
        "        activation='________',\n",
        "        input_shape=(________,),\n",
        "        name='hidden_layer_1'\n",
        "    ))\n",
        "\n",
        "    # Hidden Layer 2\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Add second hidden layer with 64 units and 'relu' activation\n",
        "    model.add(layers.Dense(\n",
        "        units=________,\n",
        "        activation='________',\n",
        "        name='hidden_layer_2'\n",
        "    ))\n",
        "\n",
        "    # Output Layer\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Add output layer with 10 units and 'softmax' activation\n",
        "    model.add(layers.Dense(\n",
        "        units=________,\n",
        "        activation='________',\n",
        "        name='output_layer'\n",
        "    ))\n",
        "\n",
        "    # TODO: STUDENT FILL IN\n",
        "    # Compile the model with same parameters as SLP\n",
        "    model.compile(\n",
        "        optimizer='________',\n",
        "        loss='________',\n",
        "        metrics=['________']\n",
        "    )\n",
        "\n",
        "    # Display model architecture\n",
        "    print(\"\\nMulti-Layer Perceptron Architecture:\")\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "29Aq6Gezhm8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 4: TRAINING AND EVALUATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_model(model, x_train, y_train, x_test, y_test, epochs=10, batch_size=128):\n",
        "    \"\"\"\n",
        "    Train a model and return training history.\n",
        "\n",
        "    Args:\n",
        "        model: Keras model to train\n",
        "        x_train, y_train: Training data and labels\n",
        "        x_test, y_test: Validation data and labels\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Training batch size\n",
        "\n",
        "    Returns:\n",
        "        keras.callbacks.History: Training history\n",
        "    \"\"\"\n",
        "    print(f\"\\nTraining {model.name}...\")\n",
        "    print(f\"Epochs: {epochs}, Batch size: {batch_size}\")\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=1  # Show progress bar\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate model performance and display results.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        x_test, y_test: Test data and labels\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"EVALUATING {model.name.upper()}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Get test accuracy\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "    # Get predictions for detailed analysis\n",
        "    y_pred = model.predict(x_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"\\nClassification Report for {model.name}:\")\n",
        "    print(classification_report(y_true_classes, y_pred_classes))\n",
        "\n",
        "    return test_accuracy, y_pred_classes, y_true_classes\n",
        "\n",
        "def plot_training_history(history_slp, history_mlp):\n",
        "    \"\"\"\n",
        "    Plot training history comparison between SLP and MLP.\n",
        "\n",
        "    Args:\n",
        "        history_slp: Training history of Single Layer Perceptron\n",
        "        history_mlp: Training history of Multi-Layer Perceptron\n",
        "    \"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Training accuracy\n",
        "    ax1.plot(history_slp.history['accuracy'], label='SLP Train', marker='o')\n",
        "    ax1.plot(history_mlp.history['accuracy'], label='MLP Train', marker='s')\n",
        "    ax1.set_title('Training Accuracy Comparison')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Validation accuracy\n",
        "    ax2.plot(history_slp.history['val_accuracy'], label='SLP Val', marker='o')\n",
        "    ax2.plot(history_mlp.history['val_accuracy'], label='MLP Val', marker='s')\n",
        "    ax2.set_title('Validation Accuracy Comparison')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Training loss\n",
        "    ax3.plot(history_slp.history['loss'], label='SLP Train', marker='o')\n",
        "    ax3.plot(history_mlp.history['loss'], label='MLP Train', marker='s')\n",
        "    ax3.set_title('Training Loss Comparison')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Loss')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # Validation loss\n",
        "    ax4.plot(history_slp.history['val_loss'], label='SLP Val', marker='o')\n",
        "    ax4.plot(history_mlp.history['val_loss'], label='MLP Val', marker='s')\n",
        "    ax4.set_title('Validation Loss Comparison')\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Loss')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrices(y_true_slp, y_pred_slp, y_true_mlp, y_pred_mlp):\n",
        "    \"\"\"\n",
        "    Plot confusion matrices for both models.\n",
        "\n",
        "    Args:\n",
        "        y_true_slp, y_pred_slp: True and predicted labels for SLP\n",
        "        y_true_mlp, y_pred_mlp: True and predicted labels for MLP\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # SLP Confusion Matrix\n",
        "    cm_slp = confusion_matrix(y_true_slp, y_pred_slp)\n",
        "    sns.heatmap(cm_slp, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
        "    ax1.set_title('Single Layer Perceptron\\nConfusion Matrix')\n",
        "    ax1.set_xlabel('Predicted Label')\n",
        "    ax1.set_ylabel('True Label')\n",
        "\n",
        "    # MLP Confusion Matrix\n",
        "    cm_mlp = confusion_matrix(y_true_mlp, y_pred_mlp)\n",
        "    sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
        "    ax2.set_title('Multi-Layer Perceptron\\nConfusion Matrix')\n",
        "    ax2.set_xlabel('Predicted Label')\n",
        "    ax2.set_ylabel('True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "OI-FHbdghoXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 5: MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the complete experiment.\n",
        "    \"\"\"\n",
        "    print(\"MNIST CLASSIFICATION: Single Layer vs Multi-Layer Perceptron\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
        "\n",
        "    # Create models\n",
        "    slp_model = create_single_layer_perceptron()\n",
        "    mlp_model = create_multi_layer_perceptron()\n",
        "\n",
        "    # Training parameters\n",
        "    EPOCHS = 15\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # Train Single Layer Perceptron\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TRAINING PHASE\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    history_slp = train_model(slp_model, x_train, y_train, x_test, y_test,\n",
        "                             epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Train Multi-Layer Perceptron\n",
        "    history_mlp = train_model(mlp_model, x_train, y_train, x_test, y_test,\n",
        "                             epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EVALUATION PHASE\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    acc_slp, pred_slp, true_slp = evaluate_model(slp_model, x_test, y_test)\n",
        "    acc_mlp, pred_mlp, true_mlp = evaluate_model(mlp_model, x_test, y_test)\n",
        "\n",
        "    # Compare results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FINAL COMPARISON\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Single Layer Perceptron Accuracy: {acc_slp:.4f} ({acc_slp*100:.2f}%)\")\n",
        "    print(f\"Multi-Layer Perceptron Accuracy:  {acc_mlp:.4f} ({acc_mlp*100:.2f}%)\")\n",
        "    print(f\"Improvement with MLP: {((acc_mlp - acc_slp)/acc_slp)*100:.2f}%\")\n",
        "\n",
        "    # Plot results\n",
        "    plot_training_history(history_slp, history_mlp)\n",
        "    plot_confusion_matrices(true_slp, pred_slp, true_mlp, pred_mlp)\n",
        "\n",
        "    # Model parameters comparison\n",
        "    print(f\"\\nModel Complexity Comparison:\")\n",
        "    print(f\"SLP Parameters: {slp_model.count_params():,}\")\n",
        "    print(f\"MLP Parameters: {mlp_model.count_params():,}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "SEacNx7ihtVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## DISCUSSION QUESTIONS FOR STUDENTS:\n",
        "\n",
        "1. Why does the Multi-Layer Perceptron perform better than the Single Layer Perceptron?\n",
        "\n",
        "2. What is the role of the hidden layers in the MLP? Why can't the SLP achieve the same performance?\n",
        "\n",
        "3. How does the number of parameters compare between the two models?\n",
        "   Is more parameters always better?\n",
        "\n",
        "4. What would happen if we used linear activation functions in the hidden layers of the MLP?\n",
        "\n",
        "5. Experiment: Try different architectures (more/fewer hidden layers, different sizes).\n",
        "   How does this affect performance?\n",
        "\n",
        "6. What are the limitations of both approaches for more complex tasks?"
      ],
      "metadata": {
        "id": "eDO8biC7hu9i"
      }
    }
  ]
}